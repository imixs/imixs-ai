###################################################
# Deployment of llama.cpp:server-cuda 
# GPU Support with CUDA
###################################################
---
kind: Deployment
apiVersion: apps/v1
metadata:
  namespace: llama-cpp
  name: server-cuda
  labels: 
    app: server-cuda
spec:
  replicas: 1
  selector:
    matchLabels:
      app: server-cuda
  template:
    metadata:
      labels:
        app: server-cuda
    spec:
      containers:
      - name: server-cuda
        image: ghcr.io/ggerganov/llama.cpp:server-cuda
        args:
        - "-m"
        - "/models/Mistral-7B-Instruct-v0.3.Q8_0.gguf"
        #- "-c"
        #- "4096"
        #- "--verbose"
        - "--host"
        - "0.0.0.0"
        - "--port"
        - "8080"
        - "--n-gpu-layers"
        - "99"        
        - "--ctx-size"
        - "16384"
        - "--n-predict"
        - "2048"
        imagePullPolicy: Always
        ports: 
          - name: web
            containerPort: 8080
        resources:
          limits:
            nvidia.com/gpu: 1
        volumeMounts:
        - mountPath: /models
          name: llama-cpp-models
      restartPolicy: Always
      volumes:
      - name: llama-cpp-models
        persistentVolumeClaim:
          claimName: llama-cpp-models

